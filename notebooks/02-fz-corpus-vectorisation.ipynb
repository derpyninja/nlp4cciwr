{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vectorise Corpus\n",
    "Felix Zaussinger | 08.01.2021\n",
    "\n",
    "## Core Analysis Goal(s)\n",
    "1. Explore textacy vectorisation functions\n",
    "2. Transform corpus to sparse matrix representations\n",
    "\n",
    "## Key Insight(s)\n",
    "1. Creating the sparse matrix format from the corpus takes long (V1 ~ 30 min, > V2 ~ 7 min)\n",
    "2. Compared to the full corpus, parse matrices are really small memory-wise\n",
    "3. There is a lot of subtlety in choosing parameters, particularly weighting schemes. I tried to follow a standard implementations of a TF-IDF weighting scheme.\n",
    "\n",
    "## Sources\n",
    "- https://textacy.readthedocs.io/en/stable/api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic commands\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import textacy\n",
    "import logging\n",
    "import numpy as np\n",
    "import textacy.vsm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import en_core_web_lg\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from textacy import preprocessing\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# module settings\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# project directory\n",
    "abspath = os.path.abspath('')\n",
    "project_dir = str(Path(abspath).parents[0])\n",
    "\n",
    "# sub-directories\n",
    "data_raw = os.path.join(project_dir, \"data\", \"raw\")\n",
    "data_interim = os.path.join(project_dir, \"data\", \"interim\")\n",
    "data_processed = os.path.join(project_dir, \"data\", \"processed\")\n",
    "model_dir = os.path.join(project_dir, \"models\")\n",
    "figure_dir = os.path.join(project_dir, \"reports\", \"figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load and configure spacy nlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 800 ms, total: 4.92 s\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = en_core_web_lg.load()\n",
    "nlp.max_length = 30000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define version/iteration ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"V6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load textacy corpus that stores the pre-processed BBC Monitoring data and its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 25s, sys: 5.48 s, total: 7min 31s\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fname_corpus = \"BBC_2007_07_04_CORPUS_TEXTACY_{}.bin.gz\".format(version)\n",
    "corpus = textacy.Corpus.load(\n",
    "    lang=nlp,\n",
    "    filepath=os.path.join(data_processed, fname_corpus),\n",
    "    store_user_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(1691 docs, 42797573 tokens)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation (textacy.vsm.vectorizers)\n",
    "\n",
    "Two key options:\n",
    "1. Vectorizer: Transform a collection of tokenized documents into a **document-term matrix of shape (# docs, # unique terms)**, with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "2. GroupVectorizer: Transform a collection of tokenized documents into a **group-term matrix of shape (# unique groups, # unique terms)**, with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "\n",
    "Further info:\n",
    "- *doc.to_terms_list* (generator function!): Transform Doc into a sequence of ngrams and/or entities — not necessarily in order of appearance — where each appears in the sequence as many times as it appears in Doc.\n",
    "- *textacy.vsm.vectorizers.GroupVectorizer*: Transform one or more tokenized documents into a group-term matrix of shape (# groups, # unique terms), with tf-, tf-idf, or binary-weighted values.This is an extension of typical document-term matrix vectorization, where terms are grouped by the documents in which they co-occur. It allows for customized grouping, such as by a shared author or publication year, that may span multiple documents, without forcing users to merge those documents themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize and vectorize the documents of this corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a generator\n",
    "tokenized_docs, basin_group, year_group = textacy.io.unzip(\n",
    "    (doc._.to_terms_list(\n",
    "        ngrams=(1, 2),\n",
    "        entities=False,\n",
    "        normalize=\"lemma\",\n",
    "        as_strings=True,\n",
    "        filter_stops=True,\n",
    "        filter_nums=True,\n",
    "        include_pos={'ADJ', 'NOUN', 'VERB'},  # also test: {'ADJ', 'NOUN', 'VERB', 'PROPN'}\n",
    "        min_freq=2,\n",
    "    )\n",
    "     , doc._.meta[\"basin\"]\n",
    "     , doc._.meta[\"year\"]) for doc in corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out terms that are too common and/or too rare (by document frequency), and compactify the top max_n_terms in the id_to_term mapping accordingly.**\n",
    "\n",
    "- min_df (float or int): If float, value is the fractional proportion of\n",
    "            the total number of documents, which must be in [0.0, 1.0]. If int,\n",
    "            value is the absolute number. Filter terms whose document frequency\n",
    "            is less than ``min_df``.\n",
    "            \n",
    "- max_df (float or int): If float, value is the fractional proportion of\n",
    "    the total number of documents, which must be in [0.0, 1.0]. If int,\n",
    "    value is the absolute number. Filter terms whose document frequency\n",
    "    is greater than ``max_df``.\n",
    "    \n",
    "- max_n_terms (int): Only include terms whose document frequency is within\n",
    "    the top ``max_n_terms``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighting schemes**\n",
    "\n",
    "\n",
    "-    “tf”: Weights are simply the absolute per-document term frequencies (tfs), i.e. value (i, j) in an output doc-term matrix corresponds to the number of occurrences of term j in doc i. Terms appearing many times in a given doc receive higher weights than less common terms. Params: tf_type=\"linear\", apply_idf=False, apply_dl=False\n",
    "\n",
    "-    “**tfidf**”: Doc-specific, local tfs are multiplied by their corpus-wide, global inverse document frequencies (idfs). Terms appearing in many docs have higher document frequencies (dfs), correspondingly smaller idfs, and in turn, lower weights. **Params: tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", apply_dl=False**\n",
    "\n",
    "-    “bm25”: This scheme includes a local tf component that increases asymptotically, so higher tfs have diminishing effects on the overall weight; a global idf component that can go negative for terms that appear in a sufficiently high proportion of docs; as well as a row-wise normalization that accounts for document length, such that terms in shorter docs hit the tf asymptote sooner than those in longer docs. Params: tf_type=\"bm25\", apply_idf=True, idf_type=\"bm25\", apply_dl=True\n",
    "\n",
    "-    “binary”: This weighting scheme simply replaces all non-zero tfs with 1, indicating the presence or absence of a term in a particular doc. That’s it. Params: tf_type=\"binary\", apply_idf=False, apply_dl=False\n",
    "\n",
    "Slightly altered versions of these “standard” weighting schemes are common, and may have better behavior in general use cases:\n",
    "\n",
    "-    “lucene-style tfidf”: Adds a doc-length normalization to the usual local and global components. Params: tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", apply_dl=True, dl_type=\"sqrt\"\n",
    "\n",
    "-    “lucene-style bm25”: Uses a smoothed idf instead of the classic bm25 variant to prevent weights on terms from going negative. Params: tf_type=\"bm25\", apply_idf=True, idf_type=\"smooth\", apply_dl=True, dl_type=\"linear\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and fit vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = textacy.vsm.GroupVectorizer(\n",
    "    tf_type=\"linear\",  # {\"linear\", \"sqrt\", \"log\", \"binary\"}\n",
    "    apply_idf=True,\n",
    "    idf_type=\"standard\",  # {\"standard\", \"smooth\", \"bm25\"}\n",
    "    apply_dl=False,\n",
    "    dl_type=\"linear\",  # {\"linear\", \"sqrt\", \"log\"}\n",
    "    norm=\"l2\",  # {\"l1\", \"l2\"} or None\n",
    "    min_df=0.3,  # Filter terms whose document frequency is less than ``min_df``\n",
    "    max_df=0.95,  # Filter terms whose document frequency is greater than ``max_df``,\n",
    "    max_n_terms=None,\n",
    "    vocabulary_terms=None, \n",
    "    vocabulary_grps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dim(matrix)** = unique groups x unique terms = 105 x 6819\n",
    "- **N = 309338** stored elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 24s, sys: 1.27 s, total: 6min 26s\n",
      "Wall time: 6min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grp_term_matrix = vectorizer.fit_transform(tokenized_docs, basin_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grp_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group-term matrix to disk (former \"step 1\" was based on a ~60% sample split)\n",
    "textacy.io.matrix.write_sparse_matrix(\n",
    "    data=grp_term_matrix,\n",
    "    filepath=os.path.join(data_processed, \"BBC_2007_07_04_CORPUS_TEXTACY_{}_GROUPTERMMATRIX_STEP1\".format(version)),\n",
    "    compressed=True  # writes to single .npz file (numpy binary format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the remaining documents of the corpus, using only the groups, terms, and weights learned in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (former \"step 2\": applied the trained model that used sample 1 to sample 2)\n",
    "#tokenized_docs, basin_group, year_group = textacy.io.unzip(\n",
    "#    (doc._.to_terms_list(ngrams=(1, 2, 3), entities=True, as_strings=True), doc._.meta[\"basin\"], doc._.meta[\"year\"]) for doc in corpus[n_split:]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_term_matrix_fitted = vectorizer.transform(tokenized_docs, basin_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group-term matrix (step 2) to disk\n",
    "textacy.io.matrix.write_sparse_matrix(\n",
    "    data=grp_term_matrix_fitted,\n",
    "    filepath=os.path.join(data_processed, \"BBC_2007_07_04_CORPUS_TEXTACY_{}_GROUPTERMMATRIX_STEP2\".format(version)),\n",
    "    compressed=True  # writes to single .npz file (numpy binary format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save trained vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(os.path.join(model_dir, 'BBC_2007_07_04_CORPUS_TEXTACY_{}_VECTORIZER.pkl'.format(version)), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the terms associated with columns and groups associated with rows (get's sorted alphabetically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(,6819)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87988131, 0.9991421 , 0.91441667, ..., 0.99871833, 0.97227946,\n",
       "       0.99617391])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_terms  # unique word counts\n",
    "vectorizer.terms_list  # terms\n",
    "textacy.vsm.matrix_utils.get_term_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_doc_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_inverse_doc_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_information_content(grp_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(105,)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akpa',\n",
       " 'Amazon',\n",
       " 'Amazonas',\n",
       " 'Amur',\n",
       " 'Araks',\n",
       " 'Aral Sea',\n",
       " 'Artibonite',\n",
       " 'Asi',\n",
       " 'Astara',\n",
       " 'Atrak',\n",
       " 'Atrek',\n",
       " 'Aviles',\n",
       " 'Awash',\n",
       " 'Ayeyarwady',\n",
       " 'Aysen',\n",
       " 'Baker',\n",
       " 'Baraka',\n",
       " 'Beilun',\n",
       " 'Belize',\n",
       " 'Benito',\n",
       " 'Bia',\n",
       " 'Bidasoa',\n",
       " 'Black River',\n",
       " 'Brahmaputra',\n",
       " 'Buzi',\n",
       " 'Ca',\n",
       " 'Candelaria',\n",
       " 'Catatumba',\n",
       " 'Catatumbo',\n",
       " 'Cavally',\n",
       " 'Cestos',\n",
       " 'Changuinola',\n",
       " 'Chico',\n",
       " 'Chiloango',\n",
       " 'Chira',\n",
       " 'Choluteca',\n",
       " 'Chu',\n",
       " 'Chui',\n",
       " 'Chuy',\n",
       " 'Coco',\n",
       " 'Colorado',\n",
       " 'Columbia',\n",
       " 'Congo',\n",
       " 'Corentyne',\n",
       " 'Coruh',\n",
       " 'Cross',\n",
       " 'Cullen',\n",
       " 'Cuvelai',\n",
       " 'Danube',\n",
       " 'Dara',\n",
       " 'Dasht',\n",
       " 'Dayan',\n",
       " 'Dnepr',\n",
       " 'Dnieper',\n",
       " 'Dniester',\n",
       " 'Dnipro',\n",
       " 'Don',\n",
       " 'Douro',\n",
       " 'Dra',\n",
       " 'Draa',\n",
       " 'Drava',\n",
       " 'Drim',\n",
       " 'Drin',\n",
       " 'Duero',\n",
       " 'Ebro',\n",
       " 'Elbe',\n",
       " 'Ertis',\n",
       " 'Etosha',\n",
       " 'Euphrates',\n",
       " 'Evros',\n",
       " 'Fane',\n",
       " 'Flurry',\n",
       " 'Fly',\n",
       " 'Foyle',\n",
       " 'Gambia',\n",
       " 'Ganges',\n",
       " 'Garona',\n",
       " 'Garonne',\n",
       " 'Garun',\n",
       " 'Gash',\n",
       " 'Golok',\n",
       " 'Grijalva',\n",
       " 'Guadiana',\n",
       " 'HIrmand',\n",
       " 'Han',\n",
       " 'Har Nuur',\n",
       " 'Hari',\n",
       " 'Harirud',\n",
       " 'Helmand',\n",
       " 'Hirmand',\n",
       " 'Hondo',\n",
       " 'Hsi',\n",
       " 'Ili',\n",
       " 'Incomati',\n",
       " 'Indus',\n",
       " 'Irrawaddy',\n",
       " 'Irtysh',\n",
       " 'Ishim',\n",
       " 'Jacobs',\n",
       " 'Jordan',\n",
       " 'Juba',\n",
       " 'Jubba',\n",
       " 'Jurado',\n",
       " 'Kabir',\n",
       " 'Kaladan',\n",
       " 'Karisu',\n",
       " 'Karnaphuli',\n",
       " 'Kebir',\n",
       " 'Komati',\n",
       " 'Kra',\n",
       " 'Krka',\n",
       " 'Kunene',\n",
       " 'Kura',\n",
       " 'La Plata',\n",
       " 'La Plate',\n",
       " 'Labe',\n",
       " 'Lake Chad',\n",
       " 'Lake Natron',\n",
       " 'Lake Rudolf',\n",
       " 'Lake Turkana',\n",
       " 'Lauca',\n",
       " 'Lempa',\n",
       " 'Liba',\n",
       " 'Lielupe',\n",
       " 'Lima',\n",
       " 'Limpopo',\n",
       " 'Ma',\n",
       " 'Mana',\n",
       " 'Maputo',\n",
       " 'Maracaibo',\n",
       " 'Mareb',\n",
       " 'Maritsa',\n",
       " 'Maroni',\n",
       " 'Marowijne',\n",
       " 'Massacre',\n",
       " 'Mataje',\n",
       " 'Mbe',\n",
       " 'Mbini',\n",
       " 'Meghna',\n",
       " 'Mekong',\n",
       " 'Mesta',\n",
       " 'Minho',\n",
       " 'Mino',\n",
       " 'Mira',\n",
       " 'Mississippi',\n",
       " 'Moa',\n",
       " 'Mono',\n",
       " 'Morro',\n",
       " 'Murgab',\n",
       " 'Murghab',\n",
       " 'Murghob',\n",
       " 'Narva',\n",
       " 'Natron',\n",
       " 'Negro',\n",
       " 'Nelson',\n",
       " 'Neman',\n",
       " 'Nemunas',\n",
       " 'Neretva',\n",
       " 'Niemen',\n",
       " 'Niger',\n",
       " 'Nile',\n",
       " 'Ntem',\n",
       " 'Nyanga',\n",
       " 'Ob',\n",
       " 'Oder',\n",
       " 'Odra',\n",
       " 'Ogooue',\n",
       " 'Oiapoque',\n",
       " 'Okavango',\n",
       " 'Oksu',\n",
       " 'Oral',\n",
       " 'Orange',\n",
       " 'Orontes',\n",
       " 'Oyapock',\n",
       " 'Paita',\n",
       " 'Palena',\n",
       " 'Parnu',\n",
       " 'Pascua',\n",
       " 'Pasvik',\n",
       " 'Patia',\n",
       " 'Paz',\n",
       " 'Pedernales',\n",
       " 'Plate',\n",
       " 'Po',\n",
       " 'Poopo',\n",
       " 'Pregel',\n",
       " 'Pregola',\n",
       " 'Pregolya',\n",
       " 'Puyango',\n",
       " 'Red',\n",
       " 'Rhine',\n",
       " 'Rhone',\n",
       " 'Rio Grande',\n",
       " 'Rovuma',\n",
       " 'Roya',\n",
       " 'Ruvuma',\n",
       " 'Sabi',\n",
       " 'Saigon',\n",
       " 'Salween',\n",
       " 'Samur',\n",
       " 'San Juan',\n",
       " 'San Martin',\n",
       " 'Saskatchewan',\n",
       " 'Save',\n",
       " 'Schelde',\n",
       " 'Scheldt',\n",
       " 'Segovia',\n",
       " 'Seine',\n",
       " 'Senegal',\n",
       " 'Sepik',\n",
       " 'Serrano',\n",
       " 'Sirdaryo',\n",
       " 'Soca',\n",
       " 'St Croix',\n",
       " 'St John',\n",
       " 'Struma',\n",
       " 'Suchiate',\n",
       " 'Sulak',\n",
       " 'Syrdarya',\n",
       " 'Tabasco',\n",
       " 'Tagus',\n",
       " 'Tajo',\n",
       " 'Taku',\n",
       " 'Tami',\n",
       " 'Tana',\n",
       " 'Tano',\n",
       " 'Tarim',\n",
       " 'Tejo',\n",
       " 'Terek',\n",
       " 'Tergi',\n",
       " 'Theiss',\n",
       " 'Tigris',\n",
       " 'Tijuana',\n",
       " 'Tisa',\n",
       " 'Tisza',\n",
       " 'Titicaca',\n",
       " 'Tobol',\n",
       " 'Topol',\n",
       " 'Tuloma',\n",
       " 'Tumbes',\n",
       " 'Tumen',\n",
       " 'Tysa',\n",
       " 'Ubsu Nuur',\n",
       " 'Umba',\n",
       " 'Umbeluzi',\n",
       " 'Ural',\n",
       " 'Uvs Nuur',\n",
       " 'Valdivia',\n",
       " 'Vardar',\n",
       " 'Vida',\n",
       " 'Vistula',\n",
       " 'Volga',\n",
       " 'Volta',\n",
       " 'Wadi Al Izziyah',\n",
       " 'Xi',\n",
       " 'Xijiang',\n",
       " 'Yafi',\n",
       " 'Yalu',\n",
       " 'Yenisey',\n",
       " 'Zambezi',\n",
       " 'Zarumilla']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_grps  # group id's\n",
    "vectorizer.grps_list  # group elements\n",
    "# textacy.vsm.matrix_utils.get_doc_lengths(grp_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the same procedure to create a year-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement\n",
    "# yr_term_matrix = vectorizer.fit_transform(tokenized_docs, year_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
