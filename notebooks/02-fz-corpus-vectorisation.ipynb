{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vectorise Corpus\n",
    "Felix Zaussinger | 08.01.2021\n",
    "\n",
    "## Core Analysis Goal(s)\n",
    "1. Explore textacy vectorisation functions\n",
    "2. Transform corpus to sparse matrix representations\n",
    "\n",
    "## Key Insight(s)\n",
    "1. Creating the sparse matrix format from the corpus takes long (at least 30 min)\n",
    "2. Compared to the full corpus, parse matrices are really small memory-wise\n",
    "3. There is a lot of subtlety in choosing parameters, particularly weighting schemes. I tried to follow a standard implementations of a TF-IDF weighting scheme.\n",
    "\n",
    "## Sources\n",
    "- https://textacy.readthedocs.io/en/stable/api_reference/vsm_and_tm.html#textacy.vsm.vectorizers.Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# magic commands\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import textacy\n",
    "import logging\n",
    "import numpy as np\n",
    "import textacy.vsm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import en_core_web_lg\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from textacy import preprocessing\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "# module settings\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(rc={'figure.figsize': (16, 9.)})\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Define directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# project directory\n",
    "abspath = os.path.abspath('')\n",
    "project_dir = str(Path(abspath).parents[0])\n",
    "\n",
    "# sub-directories\n",
    "data_raw = os.path.join(project_dir, \"data\", \"raw\")\n",
    "data_interim = os.path.join(project_dir, \"data\", \"interim\")\n",
    "data_processed = os.path.join(project_dir, \"data\", \"processed\")\n",
    "model_dir = os.path.join(project_dir, \"models\")\n",
    "figure_dir = os.path.join(project_dir, \"reports\", \"figures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load and configure spacy nlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 884 ms, total: 5.22 s\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = en_core_web_lg.load()\n",
    "nlp.max_length = 30000000\n",
    "nlp.Defaults.stop_words |= {\n",
    "    \"hyperlink\",\n",
    "    \"HYPERLINK\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load textacy corpus that stores the pre-processed BBC Monitoring data and its metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 59s, sys: 14 s, total: 16min 13s\n",
      "Wall time: 16min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fname_corpus = \"BBC_2007_07_04_CORPUS_TEXTACY.bin.gz\"\n",
    "corpus = textacy.Corpus.load(\n",
    "    lang=nlp,\n",
    "    filepath=os.path.join(data_processed, fname_corpus),\n",
    "    store_user_data=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(1510 docs, 59783406 tokens)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorisation (textacy.vsm.vectorizers)\n",
    "\n",
    "Two key options:\n",
    "1. Vectorizer: Transform a collection of tokenized documents into a **document-term matrix of shape (# docs, # unique terms)**, with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "2. GroupVectorizer: Transform a collection of tokenized documents into a **group-term matrix of shape (# unique groups, # unique terms)**, with various ways to filter or limit included terms and flexible weighting schemes for their values.\n",
    "\n",
    "Further info:\n",
    "- *doc.to_terms_list* (generator function!): Transform Doc into a sequence of ngrams and/or entities — not necessarily in order of appearance — where each appears in the sequence as many times as it appears in Doc.\n",
    "- *textacy.vsm.vectorizers.GroupVectorizer*: Transform one or more tokenized documents into a group-term matrix of shape (# groups, # unique terms), with tf-, tf-idf, or binary-weighted values.This is an extension of typical document-term matrix vectorization, where terms are grouped by the documents in which they co-occur. It allows for customized grouping, such as by a shared author or publication year, that may span multiple documents, without forcing users to merge those documents themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and vectorize the first 600 documents of this corpus:\n",
    "n_split = 700\n",
    "tokenized_docs, basin_group, year_group = textacy.io.unzip(\n",
    "    (doc._.to_terms_list(ngrams=(1, 2, 3), entities=True, as_strings=True), doc._.meta[\"basin\"], doc._.meta[\"year\"]) for doc in corpus[:n_split]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out terms that are too common and/or too rare (by document frequency), and compactify the top max_n_terms in the id_to_term mapping accordingly.**\n",
    "\n",
    "- min_df (float or int): If float, value is the fractional proportion of\n",
    "            the total number of documents, which must be in [0.0, 1.0]. If int,\n",
    "            value is the absolute number. Filter terms whose document frequency\n",
    "            is less than ``min_df``.\n",
    "            \n",
    "- max_df (float or int): If float, value is the fractional proportion of\n",
    "    the total number of documents, which must be in [0.0, 1.0]. If int,\n",
    "    value is the absolute number. Filter terms whose document frequency\n",
    "    is greater than ``max_df``.\n",
    "    \n",
    "- max_n_terms (int): Only include terms whose document frequency is within\n",
    "    the top ``max_n_terms``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighting schemes**\n",
    "\n",
    "\n",
    "-    “tf”: Weights are simply the absolute per-document term frequencies (tfs), i.e. value (i, j) in an output doc-term matrix corresponds to the number of occurrences of term j in doc i. Terms appearing many times in a given doc receive higher weights than less common terms. Params: tf_type=\"linear\", apply_idf=False, apply_dl=False\n",
    "\n",
    "-    “**tfidf**”: Doc-specific, local tfs are multiplied by their corpus-wide, global inverse document frequencies (idfs). Terms appearing in many docs have higher document frequencies (dfs), correspondingly smaller idfs, and in turn, lower weights. **Params: tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", apply_dl=False**\n",
    "\n",
    "-    “bm25”: This scheme includes a local tf component that increases asymptotically, so higher tfs have diminishing effects on the overall weight; a global idf component that can go negative for terms that appear in a sufficiently high proportion of docs; as well as a row-wise normalization that accounts for document length, such that terms in shorter docs hit the tf asymptote sooner than those in longer docs. Params: tf_type=\"bm25\", apply_idf=True, idf_type=\"bm25\", apply_dl=True\n",
    "\n",
    "-    “binary”: This weighting scheme simply replaces all non-zero tfs with 1, indicating the presence or absence of a term in a particular doc. That’s it. Params: tf_type=\"binary\", apply_idf=False, apply_dl=False\n",
    "\n",
    "Slightly altered versions of these “standard” weighting schemes are common, and may have better behavior in general use cases:\n",
    "\n",
    "-    “lucene-style tfidf”: Adds a doc-length normalization to the usual local and global components. Params: tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", apply_dl=True, dl_type=\"sqrt\"\n",
    "\n",
    "-    “lucene-style bm25”: Uses a smoothed idf instead of the classic bm25 variant to prevent weights on terms from going negative. Params: tf_type=\"bm25\", apply_idf=True, idf_type=\"smooth\", apply_dl=True, dl_type=\"linear\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and fit vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = textacy.vsm.GroupVectorizer(\n",
    "    tf_type=\"linear\",  # {\"linear\", \"sqrt\", \"log\", \"binary\"}\n",
    "    apply_idf=True,\n",
    "    idf_type=\"standard\",  # {\"standard\", \"smooth\", \"bm25\"}\n",
    "    apply_dl=True,\n",
    "    dl_type=\"linear\",  # {\"linear\", \"sqrt\", \"log\"}\n",
    "    norm=\"l2\",  # {\"l1\", \"l2\"} or None\n",
    "    min_df=0.3,  # Filter terms whose document frequency is less than ``min_df``\n",
    "    max_df=0.95,  # Filter terms whose document frequency is greater than ``max_df``,\n",
    "    max_n_terms=None,\n",
    "    vocabulary_terms=None, \n",
    "    vocabulary_grps=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **dim(matrix)** = unique groups x unique terms = 105 x 6819\n",
    "- **N = 309338** stored elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grp_term_matrix = vectorizer.fit_transform(tokenized_docs, basin_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grp_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group-term matrix (step 1) to disk\n",
    "textacy.io.matrix.write_sparse_matrix(\n",
    "    data=grp_term_matrix,\n",
    "    filepath=os.path.join(data_processed, \"BBC_2007_07_04_CORPUS_TEXTACY_GROUPTERMMATRIX_STEP1\"),\n",
    "    compressed=True  # writes to single .npz file (numpy binary format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the remaining documents of the corpus, using only the groups, terms, and weights learned in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs, basin_group, year_group = textacy.io.unzip(\n",
    "    (doc._.to_terms_list(ngrams=(1, 2, 3), entities=True, as_strings=True), doc._.meta[\"basin\"], doc._.meta[\"year\"]) for doc in corpus[n_split:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felix/Applications/miniconda3/envs/da/lib/python3.8/site-packages/textacy/vsm/vectorizers.py:682: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dl_diag = sp.spdiags(1.0 / dls, diags=0, m=n_docs, n=n_docs, format=\"csr\")\n"
     ]
    }
   ],
   "source": [
    "grp_term_matrix_fitted = vectorizer.transform(tokenized_docs, basin_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group-term matrix (step 2) to disk\n",
    "textacy.io.matrix.write_sparse_matrix(\n",
    "    data=grp_term_matrix_fitted,\n",
    "    filepath=os.path.join(data_processed, \"BBC_2007_07_04_CORPUS_TEXTACY_GROUPTERMMATRIX_STEP2\"),\n",
    "    compressed=True  # writes to single .npz file (numpy binary format)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save trained vectorizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer, open(os.path.join(model_dir, 'BBC_2007_07_04_CORPUS_TEXTACY_VECTORIZER.pkl'), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the terms associated with columns and groups associated with rows (get's sorted alphabetically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(,6819)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90847616, 0.99836367, 0.9946938 , ..., 0.87539185, 0.95176268,\n",
       "       0.89805879])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_terms  # unique word counts\n",
    "vectorizer.terms_list  # terms\n",
    "textacy.vsm.matrix_utils.get_term_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_doc_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_inverse_doc_freqs(grp_term_matrix)\n",
    "textacy.vsm.matrix_utils.get_information_content(grp_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(105,)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.03077812, 38.45391268, 34.56525858, 35.22504995, 21.29282534,\n",
       "       25.25420899, 19.04566602, 12.57434871, 14.69544105, 26.69936351,\n",
       "       25.66746539, 16.39598954, 21.04260498, 13.30715607, 31.38850336,\n",
       "       13.31433193, 34.70781504, 33.77608887, 22.10472073, 20.48401386,\n",
       "       12.92857415, 31.1367388 , 25.83822546, 27.46274337, 25.79314796,\n",
       "       18.12583756, 28.65091612, 17.09319903,  9.71803621,  9.8136983 ,\n",
       "       34.56462612, 12.70629397,  9.25719619, 36.84589357, 18.8436703 ,\n",
       "       25.38703833, 28.24437394, 35.31863087, 39.07605517, 21.47537716,\n",
       "       20.63960574, 12.58688227, 23.93995006, 18.62157725, 17.77404066,\n",
       "       32.11197819, 24.5385191 , 29.96642194, 30.09075165, 31.50935922,\n",
       "       28.68185421, 35.87362667, 25.64774347, 34.07004495, 23.90408061,\n",
       "       11.2902977 , 21.66919458, 21.60446768,  9.41885045, 22.19689998,\n",
       "        9.45960252, 11.64228112, 29.61249789, 15.2777244 , 11.21119687,\n",
       "        8.76748968,  7.77606924, 18.24751267, 24.47894776, 13.28006221,\n",
       "       24.94484306, 38.30154796, 30.99125353,  9.1159543 , 14.93805407,\n",
       "       19.36925287, 24.8231857 , 25.45143968, 10.18152124, 13.21874404,\n",
       "       35.87094406, 25.67208361, 18.82784214, 26.08953346, 28.76672696,\n",
       "       22.48424506, 24.0015695 , 14.98654984, 31.56758305, 16.14651599,\n",
       "       24.13064542, 22.78908951, 19.00551096, 33.848528  , 26.68229143,\n",
       "       13.78168813, 16.14989144, 25.48956069, 16.44485918, 21.65890602,\n",
       "       30.66606644, 26.54235359, 22.56083223, 31.46230874, 16.24547662])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_grps  # group id's\n",
    "vectorizer.grps_list[:n_max]  # group elements\n",
    "textacy.vsm.matrix_utils.get_doc_lengths(grp_term_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
